\chapter{Data}
\addcontentsline{toc}{chapter}{Data}

\section{Test Data}

We use data sets from the English-to-Czech Translation Task of the Workshop on
Statistical Machine Translation (WMT) from years 2011 to 2014.

All of these datasets consist of files with (Czech) outputs of MT systems, one
file with corresponding reference sentences and one file with the original
English source sentences. They differ in the number of MT systems and the 
length of the source files (see \Tref{wmt-data}). We perform morphological 
analysis and tagging of the MT outputs and the reference sentences using 
Morphodita \citep{morphodita}. \todo{nekde morphodita, nekde jeste morce}

\begin{table}[h]
\centering
\begin{tabular}{l|l|l|l|l}
      & systems & sentences & official score & publication\\
\hline
WMT11 & 14/10    & 3003      & “$ >= $ others”      & \cite{wmt11}  \\
WMT12 & 13          & 3003      & “$ > $ others”      & \cite{wmt12}  \\
WMT13 & 14/12    & 3000      & \textit{Expected Wins} & \cite{wmt13}  \\
WMT14 & 10          & 3003      & \textit{TrueSkill}   & \cite{wmt14}
\end{tabular}
\caption{Overview of WMT datasets. Number of systems translating from English 
to Czech (all MT systems / systems that were manually evaluated), number of 
source sentences and the official method for computing the absolute human 
judgement score.}
\label{wmt-data}
\end{table}

During the manual evaluation of WMT competitions, human judges fluent in both
the source and the target language scores five MT outputs from the best to
the worst translation. Thus, the human evaluation of MT system outputs is
available as a relative ranking of performance of five systems for~a~sentence. 

There are many ways to compute the absolute system score from this relative
ranking. The official methods for each year are presented in \Tref{wmt-data} 
and we refer to these as the \textit{gold standard}. The official method is
different for every year. Therefore, to make our evaluation internally 
consistent, we also compute another absolute score for every year using the 
“$ >$ others” method \cite{bojar-grains}, which was the WMT12 official system
score. This score is computed simply as $\frac{wins}{wins+loses} $, i.e., the 
score is~based on~how frequently the system is judged to be better than 
another system. Ties among several systems are ignored. We refer to this 
interpretation of the human judgments as \textit{silver standard} to distinguish
it from the official system scores.

The performance of an evaluation metric in MT is usually computed as Pearson 
correlation coefficient or alternatively Spearman rank correlation between the 
automatic metric and human judgment. Both~correlations estimate the linear 
dependency between two sets of values and range from -1 (perfect negative 
linear relationship) to 1 (perfect linear correlation). In our experiments, we use
Pearson as it takes into account the distances between the system scores, thus 
it should be more reliable for similarly evaluated systems 
\citep{machacek-bojar-2014-results}.

\section{Sources of Czech Paraphrases}
We use the following available sources of Czech paraphrases.

\subsection{Czech WordNet 1.9 PDT}
The first one is the Czech WordNet 1.9 PDT \cite{czech-wordnet}. It is derived from the WordNet
\cite{wordnet} by automatic translation followed by manual control. It~contains rather high quality 
lemmatized paraphrases. On~the other hand, their amount is~insufficient for our purposes (see 
\Tref{number_of_substitutions}). 

\subsection{Meteor tables} %two large-scale collections of paraphrases
\label{meteori}
Meteor tables \cite{meteor} are an additional source of paraphrases. 
They are large in~size, but they contain a lot of noise as they are constructed automatically 
from parallel data via pivoting \cite{pivoting}. 

The noise was particularly high among the multiword paraphrases -- for example: \textit{svého názoru} 
(its opinion) and \textit{šermovat rukama a mlátit neviditelného} (to flail one's arms and to beat 
the invisible one) are selected as a paraphrase. 

Among one-word paraphrases the noise is sparser, but there are still pairs like \textit{1873} - 
\textit{pijavice} (a leech) or \textit{afgh\'{a}nci} (Afghans) - \textit{š\v{t}astně} (happily) 
identified as synonyms. However, the biggest problem is that most of synonymous pairs were just 
different word forms of the same lemma. 

We therefore attempt to automatically filter the Meteor table, the methods are described in Section 
\ref{filtering-section}

\subsection{PPDB} 
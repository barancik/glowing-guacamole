\chapter{Data}
\addcontentsline{toc}{chapter}{Data}

\section{Test Data}

We use data sets from the English-to-Czech Translation Task of the Workshop on
Statistical Machine Translation (WMT) from years 2011 to 2014.

All of these datasets consist of \diff{one file with the original English source sentences,
several files with Czech outputs of MT systems and one file with corresponding 
reference sentences. Data from each year of the WMT competition} differ in the 
number of MT systems and the length of the source files (see \Tref{wmt-data}). 

%We perform morphological 
%analysis and tagging of the MT outputs and the reference sentences using 
%Morphodita \citep{morphodita}. \todo{nekde morphodita, nekde jeste morce}

\begin{table}[h]
\centering
\begin{tabular}{l|l|l|l|l}
      & systems & sentences & official score & publication\\
\hline
WMT11 & 14/10    & 3003      & “$ >= $ others”      & \cite{wmt11}  \\
WMT12 & 13          & 3003      & “$ > $ others”      & \cite{wmt12}  \\
WMT13 & 14/12    & 3000      & \textit{Expected Wins} & \cite{wmt13}  \\
WMT14 & 10          & 3003      & \textit{TrueSkill}   & \cite{wmt14}
\end{tabular}
\caption{Overview of WMT datasets. Number of systems translating from English 
to Czech (all MT systems / systems that were manually evaluated), number of 
source sentences and the official method for computing the absolute human 
judgement score.}
\label{wmt-data}
\end{table}

During the manual evaluation of WMT competitions, human judges fluent in both
the source and the target language scores five MT outputs from the best to
the worst translation. Thus, the human evaluation of MT system outputs is
available as a relative ranking of performance of five systems for~a~sentence. 

There are many ways to compute the absolute system score from this relative
ranking. The official methods for each year are presented in \Tref{wmt-data} 
and we refer to these as the \textit{gold standard}. The official method is
different for every year. Therefore, to make our evaluation internally 
consistent, we also compute another absolute score for every year using the 
“$ >$ others” method \citep{bojar-grains}, which was the WMT12 official system
score. This score is computed simply as $\frac{wins}{wins+loses} $, i.e., the 
score is~based on~how frequently the system is judged to be better than 
another system. Ties among several systems are ignored. We refer to this 
interpretation of the human judgments as \textit{silver standard} to distinguish
it from the official system scores. % chcu to tu jeste zduvodnit, proc > others??

\diff{The performance of an evaluation metric in MT is usually computed as Pearson 
correlation coefficient or alternatively Spearman rank correlation between the 
automatic metric and human judgment. }

\diff{The Pearson correlation coefficient $\rho$ is defined:}

\begin{equation*}
\rho(H,M) = \frac{ \sum_{i=1}^{n}{(H_i - \tilde{H})(M_i - \tilde{M})}}{ \sqrt{ \sum_{i=1}^{n}{(H_i - \tilde{H})^2} }  \sqrt{ \sum_{i=1}^{n}{(M_i - \tilde{M})^2} } } 
\end{equation*}

\diff{where $H$ is the vector of human scores (i.e. gold or silver standard here) and 
$M$ is the vector of corresponding scores predicted by a certain metric. 
$\tilde{H}$ a $\tilde{M}$ are their means, respectively. }

\diff{The Spearman correlation coefficient $r_{s} $ is defined as the Pearson 
correlation coefficient between the rank variables -- all human scores and  
metric’s scores of MT systems are converted into ranks $r(H)$ and $r(M)$.}

\begin{equation*}
r_{s}(H,M) = \rho(r(H),r(M))
\end{equation*}

Both~correlations estimate the linear dependency between two sets of values and 
range from -1 (perfect negative linear relationship) to 1 (perfect linear correlation). 
In our experiments, we use Pearson as it takes into account the distances between 
the system scores, thus it should be more reliable for similarly evaluated systems 
\citep{machacek-bojar-2014-results}. \todo{tu to jeste trochu rozsirit, kdyz uz
to tu je vysvetleny}

%
%WMT13
%We measured the quality of system-level metrics’ scores using the Spearman’s rank correlation coefficient $\rho$. For each direction of translation we converted the official human scores into ranks. For each metric, we converted the metric’s scores of systems in a given direction into ranks. Since there were no ties in the rankings, we used the simplified formula to compute the Spearman’s $\rho$. 
%
%\begin{equation*}
%r_{s} = 1 -  \frac{6\sum{d_i^{2}}}{n(n^{2}-1)} 
%\end{equation*}
%where $d_{i}$ is the difference between the rank for system$_{i}$ and n is the number of systems. The possible values of range between 1 (where all systems are ranked in the same order) and -1 (where the systems are ranked in the reverse order). 
%
%\begin{equation*}
%r_{s} = r_{p}(r(H_{i}),r(M_{i}))
%\end{equation*}

\section{Sources of Czech Paraphrases}
We use the following available sources of Czech paraphrases.

\subsection{Czech WordNet 1.9 PDT}
The first one is the Czech WordNet 1.9 PDT \citep{czech-wordnet}. It is derived 
from the WordNet \cite{wordnet} by automatic translation followed by manual 
control. It~contains rather high quality lemmatized paraphrases. Unfortunately, 
their amount is~insufficient for our purposes (see \todo{tabulka s velikostmi}). 

\todo{A modified version of the Czech Wordnet used to annotate "The Lexico-Semantic Annotation of PDT using Czech WordNet". The Czech WordNet was developed by the Centre of Natural Language Processing at the Faculty of Informatics, Masaryk University, Czech Republic. The Czech WordNet captures nouns, verbs, adjectives, and partly adverbs, and contains 23,094 word senses (synsets). 203 of these were created or modified by UFAL during correction of annotations. This version of WordNet was used to annotate word senses in PDT.}

\subsection{Meteor tables} %two large-scale collections of paraphrases
\label{meteori}
Meteor tables \citep{meteor-tables} are an additional source of paraphrases. 
They are large in~size, but they contain a lot of noise as they are constructed automatically 
from parallel data via pivoting \citep{pivoting}. 

The noise is particularly apparent among the multiword paraphrases -- for 
example: \textit{svého názoru}  (its opinion) and \textit{šermovat rukama a 
mlátit neviditelného} (to flail one's arms and to beat the invisible one) are 
selected as a paraphrase. 

Among one-word paraphrases the noise is sparser, but there are still pairs like 
\textit{1873} - \textit{pijavice} (a leech) or \textit{afgh\'{a}nci} (Afghans) - 
\textit{š\v{t}astně} (happily) identified as synonyms. 

\todo{However, the biggest problem is that most of synonymous pairs were just 
different word forms of the same lemma. We therefore attempt to automatically 
filter the Meteor table, the methods are described in Section \ref{filtering-section}}

\subsection{PPDB} 
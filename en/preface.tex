\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


Mr. Praline: 'E's not pinin'! 'E's passed on! This parrot is no more! He has ceased to be! 'E's expired and gone to meet 'is maker! 'E's a stiff! Bereft of life, 'e rests in peace! If you hadn't nailed 'im to the perch 'e'd be pushing up the daisies! 'Is metabolic processes are now 'istory! 'E's off the twig! 'E's kicked the bucket, 'e's shuffled off 'is mortal coil, run down the curtain and joined the bleedin' choir invisible!! THIS IS AN EX-PARROT!! \cite{parrot}

Above is probably the most famous paraphrases example in the popular culture. 
It represents very well the richness of natural languages, how many different forms are there to express a simple fact of a bird demise. 
In fact, the Wikipedia registers over 100 expressions related to death -- mostly idioms, euphemisms and slang.  \citep{wiki:death} 
While the language speakers may enjoy this language richness in poetry, puns or even normal conversation, it represents rather complication for many automatic language processing tasks, e.g., natural language inference (NLI).
In NLI \footnote{formerly known as recognizing textual entailment (RTE)}, an algorithm is given two texts - a \emph{premise} and a \emph{hypothesis} and determines whether the meaning of the hypothesis can be inferred from the premise based on their semantic similarity. \citep{dagan:2005}
% determining whether a "hypothesis" is true (entailment), false (contradiction), or undetermined (neutral) given a "premise".

\todo{Příklad z MultiNLI.}

'Less loud.     Please be quiet.        85540   85540e  fiction entailment
Stop blaming John.      Quit holding John responsible.
And no one will be nicer.       And not a single person will act in a more kind manner.


Similarly, it can cause problems in automatic machine translation evaluation. 
During automatic machine translation evaluation, a \emph{hypothesis} (the output of automatic translator) is compared to a \emph{reference sentence} (translation of original sentence by a professional translator). 
However, \citep{bojar-scratching} shows that there are might be hundreds of thousands correct translations of a single Czech sentence.
Most common evaluation metrics such as BLUE \citep{bleu} disregards synonymous expressions and word order variants.
A machine translation system can be thus penalized for even perfectly translated sentence, see \Fref{example_of_BLEU_malfunction}

\begin{figure*}[h]
\begin{center}
\begin{tabular}{ll}
 Original sentence &  \begin{tabular}{l}
  	\textit{Banks are testing payment by mobile telephone} \\
	\end{tabular}  \\
 \hline
 
 Hypothesis & \begin{tabular}{llllll}
 			\textit{Banky} & \textit{zkou\v sej\'i} & \textit{platbu} & 
 			\textit{pomoc\'i} & \textit{mobiln\'iho} & \textit{telefonu} \\
 			Banks & are testing & payment & with help & mobile & phone \\
			\end{tabular} \\
 & \begin{tabular}{l}
  	Banks are testing payment by mobile phone \\
	\end{tabular} \\

 \hline
 Reference sentence & \begin{tabular}{llll}
 			\textit{Banky} & \textit{testuj\'i} & \textit{placen\'i} & 
 			\textit{mobilem} \\
 			Banks & are testing & paying & by mobile phone \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	Banks are testing paying by mobile phone \\
	\end{tabular}
 
\end{tabular}
\caption{Example from WMT12 \citep{wmt12} - 
Even though the hypothesis is grammatically correct and the meaning of both sentences is the same, there is only one unigram overlapping. 
The hypothesis would score poorly using majority of automatic evalation metrics.}
\end{center}
\label{example_of_BLEU_malfunction}
\end{figure*}


This is especially problematic for~morphologically rich languages with free word order like the Czech language.  %\cite{bojar-tackling-sparse-data}.

There are two main ways how to deal with this problem. 
Firstly, this penalization can be softened using more reference sentences.
However, the standard practice is using only one reference sentence as creating reference sentences is both time-consuming and costly.
The other method is using a machine translation evaluation metric that takes into account synonymous phrases and word form variants. 
There is one such a metric available for the Czech language - METEOR. \cite{meteor} 
Unfortunately as we show further its paraphrase sources are so noisy, it awards even sentences with paraphrases that are not grammatically correct, or are partly untranslated.

In this thesis, we l take a different approach around this problem. 
As the main task of an MT metric is essentially to decide whether a hypothesis is a paraphrase of a given reference sentence, we aim to achieve higher accuracy of machine translation evaluation by targeted paraphrasing of reference sentences. In other words, we explore the possibility of transforming the reference sentence automatically to a new one that is still grammatically correct and keeps its original meaning but at the same time it is closer in wording to the hypothesis.
BLEU and other string-based metrics should perform more reliable using these new custom-made references.


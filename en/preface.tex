\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


\chapquote{'E's not pinin'! 'E's passed on! This parrot is no more! He has ceased to be! 'E's expired and gone to meet 'is maker! 'E's a stiff! Bereft of life, 'e rests in peace! If you hadn't nailed 'im to the perch 'e'd be pushing up the daisies! 'Is metabolic processes are now 'istory! 'E's off the twig! 'E's kicked the bucket, 'e's shuffled off 'is mortal coil, run down the curtain and joined the bleedin' choir invisible!! THIS IS AN EX-PARROT!!}{Mr. Praline}{\cite{parrot}}


The paragraph above is a well-known scene from Monty Python's Parrot Sketch, which is arguably one of the most famous examples of paraphrasing in popular culture. This scene effectively illustrates the extensive variety found within natural languages, showcasing the numerous ways in which a simple fact of a bird's demise can be expressed. It highlights the richness and diversity inherent in language usage.

In fact, Wikipedia registers over 100 expressions related to death -- mostly idioms, euphemisms and slang.  \citep{wiki:death} While language speakers may appreciate and enjoy this abundance of linguistic richness in various contexts such as poetry, puns, or casual conversations, a wide range of expressions and variations related to a particular concept can make it more difficult for automated systems to precisely interpret and comprehend the intended meaning. Paraphrases remain a challenge for various automatic language processing tasks, including information retrieval, sentiment analysis, or natural language inference (NLI).

In NLI\footnote{formerly known as recognizing textual entailment (RTE)}, an algorithm is provided with two texts - a \emph{premise} and a \emph{hypothesis}. The task is to determine whether the meaning of the hypothesis can be inferred from the premise, based on their semantic similarity. The algorithm aims to assess the relationship between the two texts, typically categorized as either \emph{entailment}, \emph{contradiction}, or \emph{neutral}. The objective is to capture the logical inference or lack thereof between the premise and the hypothesis using computational methods and semantic understanding. \citep{dagan:2005} 
\todo{tu neco chybi!! proc jsou parafraze problematicke v NLI??? jako ze lexikalni metody nestaci}


\begin{figure*}[h]
\begin{center}
\begin{tabular}{ll}
 Less loud.   &  Please be quiet. \\
  \hline
Stop blaming John.  &    Quit holding John responsible. \\
 \hline
And no one will be nicer.   &    And not a single person will act in a more kind manner. \\
\end{tabular}
\caption{Example of entailments from MultiNLI \citep{williams-etal-2018-broad}, the large dataset specifically designed to facilitate the development and evaluation of sentence understanding machine learning models.}
\label{nli_examples}
\end{center}
\end{figure*}


\begin{figure*}[h]
\begin{center}
\begin{tabular}{ll}
 Source  &  \begin{tabular}{l}
  	\textit{Banks are testing payment by mobile telephone} \\
	\end{tabular}  \\
 \hline
 
 Hypothesis & \begin{tabular}{llllll}
 			\textit{Banky} & \textit{zkou\v sej\'i} & \textit{platbu} & 
 			\textit{pomoc\'i} & \textit{mobiln\'iho} & \textit{telefonu} \\
 			Banks & are testing & payment & with help & mobile & phone \\
			\end{tabular} \\
 & \begin{tabular}{l}
  	Banks are testing payment by mobile phone \\
	\end{tabular} \\

 \hline
 Reference  & \begin{tabular}{llll}
 			\textit{Banky} & \textit{testuj\'i} & \textit{placen\'i} & 
 			\textit{mobilem} \\
 			Banks & are testing & paying & by mobile phone \\
			\end{tabular} \\
 &  \begin{tabular}{l}
  	Banks are testing paying by mobile phone \\
	\end{tabular}
 
\end{tabular}
\caption{In this example from WMT12 \citep{wmt12}, despite the hypothesis being grammatically correct and having the same meaning as the source sentence, there is only a single overlapping unigram between them. As a result, the hypothesis would receive a very low score when evaluated using n-gram overlap-based automatic evaluation metrics.}
\label{example_of_BLEU_malfunction}
\end{center}
\end{figure*}

Similarly, automatic machine translation evaluation can encounter challenges due to the potential variability and diversity of correct translations for a single source sentence. During the evaluation process, a hypothesis (the output of the automatic translator) is typically compared to a reference sentence, which is the translation of the original sentence done by a professional translator.

However, \citep{bojar-scratching} shows that there are might be hundreds of thousands correct translations of a single Czech sentence. Many commonly used evaluation metrics, such as BLEU \citep{bleu}, fail to consider synonymous expressions and word order variants. As a result, even if a machine translation system produces a perfectly translated sentence, it may still be penalized by the evaluation metrics. \Fref{example_of_BLEU_malfunction} illustrates this issue, where a machine translation that is correct and accurate could receive a lower evaluation score due to the metrics' limitations in capturing semantic equivalence. This is especially problematic for~morphologically rich languages with free word order like the Czech language. \cite{bojar-tackling-sparse-data}

There are several ways how to tackle this problem. Firstly, this penalization can be softened using more reference sentences. While many automatic machine translation evaluation metrics allow for the use of multiple reference translations, the standard practice is to utilize only a single reference sentence. This approach is prevalent due to practical considerations, as gathering multiple high-quality reference translations for each source sentence can be time-consuming and resource-intensive. \todo{citace}

The other way around is using a machine translation evaluation metric that employs other semantic resources such as paraphrases or embeddings. For the Czech language there is only one metric that takes into account synonymous phrases and word form variants, METEOR. \cite{meteor-wmt:2014}  Unfortunately as we show further its paraphrase sources are so noisy, it awards even sentences with paraphrases that are not grammatically correct, or are partly untranslated.

\todo{Tu uz je to pomerne zastarale, chybi neuronove evaluacni metody, jako treba Comet}

\cite{marie-etal-2021-scientific} highlighted an intriguing phenomenon within the field of machine translation evaluation. Despite the development of more than 100 new evaluation metrics (e.g., \cite{rei-etal-2020-comet}, \cite{BERTScore}, \cite{kocmi-federmann-2023-large} )between the years 2010-2021, a staggering 99\% of the research papers in machine translation continue to rely on BLEU to evaluate translation quality and rank systems. Remarkably, many of these newer metrics have demonstrated superior performance compared to BLEU, yet only two of them - RIBES \citep{ribes:isozaki-etal-2010-automatic} and chrF \citep{popovic-2015-chrf} have been used in more than two research publications. Meanwhile, the traditional metrics of BLEU \citep{bleu}, TER \citep{ter}, and METEOR \citep{meteor-wmt:2014} maintain their stronghold as the most frequently employed evaluation measures within the field. \todo{to platilo v 2021, plati to jeste?}

\todo{tu preformulovat}
Many recent papers, eg. () show that BLEU is unsufficient metric for MT evaluation, so why is it still among the most common metrics? \todo{citace} show the following main reasons: comparability with previous models

\cite{mathur-etal-2020-tangled} urge to stop using BLEU, TER and similar surface metrics created two decades ago.

%%% sem by chtel ten uvod z Towards - hlavne toho Marii
 which were created two decades ago, a trend that may allegedly have worsened in recent times. These surface-level metrics cannot (even) measure semantic similarity
of their inputs and are thus fundamentally flawed, particularly when it comes to assessing the quality of recent state-of-the-art MT systems (e.g. Peyrard 2019; Freitag et al. 2022), raising
concerns about the credibility of the scientific field. We argue that the potential reasons for this neglect of recent high-quality metrics include: (i) non-enforcement by reviewers; (ii) easier
comparison to previous research, for example, by copying BLEU-based results from tables of related work (potentially a pitfall in itself); (iii) computational inefficiency to run expensive new
metrics at large scale; (iv) lack of trust in and transparency of high-quality black box metrics. In this work, we concern ourselves with the last-named reason, and address the explainability
of such metrics.

%Tady zminit, ze uz jsou lepsi bLack box metriky, ale v praxi se moc nepouzivaji. (MArie) PRoc - jsou nevysvetlitelne... (Towards)


In this thesis, wetake a different approach around this problem. 
As the main task of an MT metric is essentially to decide whether a hypothesis is a paraphrase of a given reference sentence, we aim to achieve higher accuracy of machine translation evaluation by targeted paraphrasing of reference sentences. In other words, we explore the possibility of transforming the reference sentence automatically to a new one that is still grammatically correct and keeps its original meaning but at the same time it is closer in wording to the hypothesis.
BLEU and other string-based metrics should perform much more reliable using these new custom-made references.


\chapter{Background: Machine Translation Evaluation}
\addcontentsline{toc}{chapter}{Evaluation}

\section{Human Evaluation}
Since the first appearance of machine translation systems, there has been a necessity for their objective evaluation and comparison. Direct human judgement approach is widely considered to be the most reliable and is the preferred method for MT system developers. 

There are many ways how to judge the quality of a machine translation system. 
The methods range from scoring fidelity and intelligibility of a translation to the ability to perform a~task based on the automatically translated instruction, e.g., how successfully are technicians in  helicopter maintenance based on translated manual \cite{Sinaiko}.

In the first year of WMT shared tasks \citep{koehn-monz:2006:WMT}, the evaluation of machine translation quality was approached from two distinct angles - fluency and adequacy. The first perspective, fluency, evaluates the degree to which the translation adheres to target language norms, considering factors like grammaticality and clarity. This evaluation does not take the source text into account and requires experts fluent only in the target language. In contrast, accuracy assesses the adherence of the translation to source text meaning, focusing on how well the target text represents the informational content of the source text. Evaluators for accuracy have access to both the source text and the translations under consideration. \todo{preformulovat, takhle to bylo vykradene}
In this method, initially  developed by the Linguistics Data Consortium \todo{(LDC,2005)}, a human expert is presented with five hypotheses and assign them a score between 1 and 5 based on how well is the sentence formed in the target language and how well the original meaning of the sentence is sustained.
The evaluators must be bilingual in both the source and target languages. 

\cite{callison-burch-etal-2007-meta} reports high correlation between annotators’ adequacy and fluency scores. In other words -- it is hard to draw any meaning from a disfluent sentence and similarly, for complete understanding of a sentence, it must be almost fully fluent as minor variation in morphology can significantly modify meaning of a whole sentence, especially in flective languages such as the Czech language.  Furthermore, the separation of adequacy and fluency leads to the problem of recombining the two scores in some meaningful way for tasks such as tuning automatic metrics. 

The very next year this method was replaced by relative ranking method (RR) \citep{callison-burch-etal-2007-meta}. In this evaluation campaign, an annotator see five hypotheses and ranks them relatively from the best to the worst with ties allowed. The judges are not given explicit instructions to evaluate specific aspects of the translation system, such as fluency or adequacy. Instead, they are asked only to rank translations by their “quality” where they leave up to the judge to decide what quality means.

% The numbers do not represent the absolute measure of quality, but a “rank” of translation: rank 1 means that the translation is the best translation out of the 5 offered translations, while rank 5 means that this is the worst translation. Ties in rankings are allowed.
%The reason why the judge is offered 5 translations to rank instead of just 2 is because judging ranks of 5 translations at the same time gives 10 pairwise rankings. Because this causes more cognitive effort and less reliable rankings some other translation tasks present only 2 translations to the judges (Braslavski et al., 2013).

This absence of exact rules makes very good sense -- it acknowledges that the annotator, being a native speaker of the evaluated language,  is capable of making a choice based on their own subjective judgment without requiring explicit guidelines. On the other hand, this lack of rules makes the task possibly even more challenging, as illustrated in\Tref{table:wmt15:example}, where the Russian source sentence was translated differently by all 14 participating MT systems.

\begin{table}[htb]
    \centering
    \begin{tabular}{l}
Source: Митинг закончился возложением цветов к закладному камню.\\
Reference: The rally ended with the placing of flowers at the foundation stone. \\
\hline
1. The meeting ended with the laying of flowers of the chapel stone. \\
2. The meeting ended with the laying of flowers to stone. \\
3. Rally ended Bill of sale laying flowers to stone. \\
4. The meeting ended with the laying on of colours to stone. \\
5. The rally ended with laying flowers to the zakladnomu stone. \\
6. The meeting ended with the laying of flowers to the foundation stone. \\
7. Meeting ended with the placing of colors to the mortgage stone. \\
8. The rally ended with laying flowers to the memorial stone.\\
9. Meeting ended with flower-laying to the foundation stone. \\
10. The meeting ended with laying flowers to clad stone.\\
11. The rally ended laying flowers to slaid stone.\\
12. Rally ended laying flowers to закладному stone.\\
13. The meeting was laying flowers to закладному stone.\\
\end{tabular}
     \caption{Example from WMT15 \cite{wmt15}, Russian to English translation task.}
    \label{table:wmt15:example}
\end{table}
\todo{proverit - pisu 14 prekladovych systemu, mam jen 13 prekladu}

Consider the task of comparing the aforementioned statements. What should be considered worse in terms of their undesirable attributes? Nonsensical sentences, untranslated words, poorly translated words, shifts in sentence meaning, grammatical errors, or missing articles? Determining which of these attributes should be considered more severe is subjective and varies among annotators based on their linguistic sensibility.

All these intricacies involved in translation evaluation lead to low inter-annotator agreement \cite{wmt13}. \todo{updatovat citace} Furthermore, after seeing several complex translations and making the difficult choice in their comparison, it is literally impossible to remember one's choices, which makes manual annotation unreproducible and inconsistent. Additionally, a person's opinion of their quality changes during the evaluation, resulting in low inter-annotator agreement. \todo{odkaz}

Furthermore, it has other major drawbacks - it is slow and expensive. For these reasons, it is literally impossible to use human evaluation for incremental tuning and development of MT systems. Well-performing automatic MT evaluation metrics are essential precisely for these tasks. 

\todo{nasledujici odstavec asi budu muset taky zminit, ze nebylo jen RR, ale DA}
%In recent years there have been a few publications that recommend using a more modern version of absolute scoring of translation instead of relative scoring. Graham et al. (2013) propose usage of a continuous scale that can assign integer numbers between 1 and 100 which gives a much higher granularity to the judgment than the previous 5 integers scale. Together with large number of judges per sentence, the average of the these absolute scores can give more reliable judgments than relative ranking approach. This method of directly assigning the absolute adequacy score to the system, named direct assessment (DA), was used in parallel with RR method in human evaluation on WMT16 shared translation task (Bojar et al., 2016).
%Human judges judge only individual sentences. The procedure for generalizing the judgments of individual sentences to the judgments of systems that generated these sentences is varies depending on the type of judgments that are collected. In the case of DA judgments the procedure is easy: we just need to take the average of the absolute scores assigned by human judges to the sentences produced by the system.
%In case RR judgments there are several possible procedures that were changing over the years. Initial measures counted the number of wins of one system versus any other system that was compared against it. The formula for computing system’s score is given bellow where win(Si, Sj ) is the number of judgments in which system Si was ranked as better than system Sj and the ties are ignored:
%��j,j̸=i win(Si,Sj)
%score(Si)= ��j,j̸=iwin(Si,Sj)+win(Sj,Si) (2.32)
%This method was used in the initial WMT tasks, but it was changed because it suffers from the problems of luck-of-the-draw — system that is less “lucky” might always get compared to the strong MT systems and unfairly get a low score.
%To account for this a new scoring method called expected wins (Koehn, 2012) was used instead:
 
%30 Chapter 2. Background: Statistical Machine Translation
%score(Si) = 1 �� win(Si,Sj) (2.33) |{Sj}|j,j̸=i win(Si,Sj)+win(Sj,Si)
%This method computes the expectation that the system in question Si will have a win over any other system randomly drawn from the set of systems it was compared against.
%Recent WMT tasks use more sophisticated measures such as TrueSkill (Sakaguchi et al., 2014), which give more reliable system scores (Bojar et al., 2014) but are much more complicated and computationally involved than Expected Wins scoring.

\section{Automatic Evaluation}
The main objective of automatic evaluation metrics is to calculate a numerical 
score reflecting quality of the translation. The better the metric, the higher it
should agree (correlate) with manually made human judgment. 

While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are an imperfect substitute for human assessment of translation quality. 

Reference based metrics have 


\todo{tohle vsechno je Marie}
An evaluation with automatic metrics has the advantages to be faster, more reproducible, and cheaper than an evaluation conducted by humans. This is especially true for the evaluation of machine translation. For a human evaluation, we would ideally need expert translators. For many language pairs, such experts are extremely rare and difficult to hire.

A large-scale and fast manual evaluation, as required by the very dynamic research area of machine translation to evaluate new systems, is often impractical. Consequently, automatic evaluation for machine translation has been a very active, and productive, research area for more than 20 years.

While BLEU remains by far the most used evaluation metric, there are countless better alternatives. BLEU is a metric from another age.

Since 2010, 100+ automatic metrics have been proposed to improve machine translation evaluation.

In this article, I present the most popular metrics that are used as alternatives, or in addition, to BLEU. I grouped them into two categories: traditional or neural metrics, each category having different advantages.
Automatic Metrics for Machine Translation

Most automatic metrics for machine translation only require:

    The translation hypothesis generated by the machine translation system to evaluate
    At least one reference translation produced by humans
    (Rarely) the source text translated by the machine translation system

Here is an example of a French-to-English translation:

    Source sentence:

Le chat dort dans la cuisine donc tu devrais cuisiner ailleurs.

    Translation hypothesis (generated by machine translation):

The cat sleeps in the kitchen so cook somewhere else.

    Reference translation:

The cat is sleeping in the kitchen, so you should cook somewhere else.

The translation hypothesis and the reference translation are both translations of the same source text.

The objective of the automatic metric is to yield a score that can be interpreted as a distance between the translation hypothesis and the reference translation. The smaller the distance is and the closer the system is to generating a translation of human quality.

The absolute score returned by a metric is usually not interpretable alone. It is almost always used to rank machine translation systems. A system with a better score is a better system.

In one of my studies (Marie et al., 2021), I showed that almost 99% of the research papers in machine translation rely on the automatic metric BLEU to evaluate translation quality and rank systems, while more than 100 other metrics have been proposed during the last 12 years. Note: I looked only at research papers published from 2010 by the ACL. Potentially many more metrics have been proposed to evaluate machine translatio
Tu vysvetlit reference, hypothesis, etc...

\subsection{Traditional}
\todo{vykradena Marie, upravit}
Traditional machine translation evaluatuin metric is based on string similarity between a reference transaltion 

Machine translation evaluation can be approached through metrics that assess the similarity between two strings based on their constituent characters. These strings consist of the translation hypothesis and the reference translation. One of the commonly used metrics in the past was WER (Word Error Rate), which was later succeeded by BLEU in the early 2000s.



Evaluation of machine translation can involve metrics that gauge the likeness between two strings, considering their component characters. These strings comprise the translation hypothesis and the reference translation. A notable historical metric is WER (Word Error Rate), which was subsequently replaced by BLEU in the early 2000s.


Advantages:

    Low Computational Cost: Traditional metrics often rely on efficient string-matching algorithms at the character and/or token level. While some metrics might involve token shifting, which could be more resource-intensive for long translations, their calculation is easily parallelizable and doesn't require GPU processing.
    Explainable: These metrics are usually straightforward to compute manually for short segments, aiding in analysis. However, this does not necessarily mean the scores are interpretable in terms of translation quality.
    Language Independence: With a few exceptions, most traditional metric algorithms can be applied across different languages without modification.

Disadvantages:

    Poor Correlation with Human Judgments: This is the main drawback compared to neural metrics. Traditional metrics do not provide a reliable estimation of translation quality.
    Specific Preprocessing Required: Except for chrF, all the traditional metrics discussed here require tokenized input segments and their reference translations. Tokenization is not integrated into the metric and must be done externally. The resulting scores are therefore reliant on specific tokenization practices that might not be consistent.

It's important to note that traditional metrics might not provide the most accurate reflection of translation quality and are being surpassed by more advanced neural metrics in this regard.




\subsection{BLEU}
BLEU (BiLingual Evaluation Understudy; \cite{bleu}) works from the hypothesis that, even though there are many different
ways how to translate a text "correctly", most of them will share certain phrases in common. If this is correct, then it should
be possible to model statistically a quasi ideal of that text against which translations can be compared in relatively simple 
string-by-string matches. 


%The pioneer metrics correlating well with human judgment were BLEU \cite{bleu} and NIST \cite{nist}. 
%They are computed based on n-gram overlap between the hypothesis and one or more corresponding reference sentences, i.e., translations made by a human translator.
% For this reasons, automatic evaluation metrics are essential for objective evaluation during development of a MT system as well as comparison of different MT systems.




Due to its simplicity and language independence, BLEU still remains the de facto standard metric for MT evaluation and tuning, even though other, better-performing metrics exist (\cite{wmt13-metrics}, \cite{wmt14}).
Even though its correlation with human judgment is not as high as previously thought (\cite{callison2006re},\cite{koehn-monz:2006:WMT}??). 
This is particular valid at a sentence level (Blatz et al. 2003).

Furthermore, the standard practice is using only one reference sentence and BLEU  then tends to~perform badly. 
There are many translations of a single sentence and even a perfectly correct translation might get a low score as BLEU disregards synonymous expressions and word order variants.
This is especially valid for~morphologically rich languages with free word order like the Czech language \cite{bojar-tackling-sparse-data}.


%Nowadays, automatic evaluation methods play a very important role in the development cycle of Machine Translation (MT) systems. They are used during error analysis to obtain preliminary translation quality reports over individual test cases. They are also a key ingredient in the process of system optimization, guiding the adjustment of internal system parameters. Finally, they allow for system comparison, both from the perspective of system improvement (i.e. comparison of different versions of the same system) and system competitiveness (i.e. comparison of different systems).

\cite{callison2006re} shows that correlation of BLUE with human judgment is not as high as previously thought. 
Also commonly used comparing MT systems based on their BLEU score turned out to be much rather problematic \cite{post-2018-call}

Furthermore, obtaining reference sentences is labour intensive and expensive,\footnote{For example, production of reference translation at~the Linguistic Data Consortium is complicated process involving translation by~professional agencies based on elaborate guidelines and detailed quality control \cite{strassel}.} thus the standard practice is using only one reference sentence and BLEU then tends to perform badly. 

As there are many translations of a single sentence, even a perfectly correct hypothesis might get a low score due different wording and disregarding synonymous expressions (see \Fref{example_of_BLEU_malfunction}). 
This is especially valid for morphologically rich languages with free word order like the Czech language. \cite{bojar-tackling-sparse-data}





\section{Meta-evalution}

We use data sets from the English-to-Czech Translation Task of the Workshop on
Statistical Machine Translation (WMT) from the years 2011 to 2014.

All of these datasets consist of one file with the original English source sentences,
several files with Czech hypotheses (outputs of MT systems) and one file with corresponding 
reference sentences. Data from each year of the WMT competition differ in the 
number of MT systems and the length of the source files (see \Tref{wmt-data}). 

%We perform morphological 
%analysis and tagging of the MT outputs and the reference sentences using 
%Morphodita \citep{morphodita}. \todo{nekde morphodita, nekde jeste morce}

\begin{table}[h]
\centering
\begin{tabular}{l|l|l|l|l}
      & systems & sentences & official score & publication\\
\hline
WMT11 & 14/10    & 3003      & “$ >= $ others”      & \cite{wmt11}  \\
WMT12 & 13          & 3003      & “$ > $ others”      & \cite{wmt12}  \\
WMT13 & 14/12    & 3000      & \textit{Expected Wins} & \cite{wmt13}  \\
WMT14 & 10          & 3003      & \textit{TrueSkill}   & \cite{wmt14}
\end{tabular}
\caption{Overview of WMT datasets. Number of systems translating from English 
to Czech (all MT systems / systems that were manually evaluated), number of 
source sentences and the official method for computing the absolute human 
judgement score.}
\label{wmt-data}
\end{table}

During the manual evaluation of WMT competitions, human judges fluent in both
the source and the target language scores five hypotheses from the best to
the worst translation. Thus, the human evaluation of hypotheses is available 
as~a~relative ranking of performance of five systems for~a~sentence. 

There are many ways to compute the absolute system score from this relative
ranking. The official methods for each year are presented in \Tref{wmt-data} 
and we refer to these as the \textit{gold standard}. The official method is
different for every year. Therefore, to make our evaluation internally 
consistent, we also compute another absolute score for every year using the 
“$ >$ others” method \citep{bojar-grains}, which was the WMT12 official system
score. This score is computed simply as $\frac{wins}{wins+loses} $, i.e., the 
score is~based on~how frequently the system is judged to be better than 
other systems, ties among several systems are ignored. We refer to this 
interpretation of human judgments as a \textit{silver standard} to distinguish
it from the official system scores. % todo: chcu to tu jeste zduvodnit, proc > others??

The performance of an evaluation metric in MT is commonly computed as the
Pearson correlation coefficient or alternatively as the Spearman rank 
correlation between the automatic metric and human judgment. 

The Pearson correlation coefficient $\rho$ is defined by the following formula:

\begin{equation*}
\rho(H,M) = \frac{ \sum_{i=1}^{n}{(H_i - \tilde{H})(M_i - \tilde{M})}}{ \sqrt{ \sum_{i=1}^{n}{(H_i - \tilde{H})^2} }  \sqrt{ \sum_{i=1}^{n}{(M_i - \tilde{M})^2} } } 
\end{equation*}

where $H$ is the vector of human scores (i.e. gold or silver standard here) and 
$M$ is the vector of corresponding scores predicted by a certain metric. 
$\tilde{H}$ a $\tilde{M}$ are their means, respectively. 

The Spearman correlation coefficient $r_{s} $ is defined as the Pearson 
correlation coefficient between the rank variables -- all human scores and  
metric’s scores of MT systems are converted into ranks $r(H)$ and $r(M)$.

\begin{equation*}
r_{s}(H,M) = \rho(r(H),r(M))
\end{equation*}

Both~correlations estimate the linear dependency between two sets of values and 
range from -1 (perfect negative linear relationship) to 1 (perfect linear correlation). 

In our experiments, we use the Pearson correlation coefficient as it takes into 
account the distances between the system scores, thus it should be more 
reliable for similarly evaluated systems \citep{machacek-bojar-2014-results}. 
\todo{tu to jeste trochu rozsirit, kdyz uz
to tu je vysvetleny - ze Spearman ignoruje jak moc jsou systemy od sebe vzdaleny}

%
%WMT13
%We measured the quality of system-level metrics’ scores using the Spearman’s rank correlation coefficient $\rho$. For each direction of translation we converted the official human scores into ranks. For each metric, we converted the metric’s scores of systems in a given direction into ranks. Since there were no ties in the rankings, we used the simplified formula to compute the Spearman’s $\rho$. 
%
%\begin{equation*}
%r_{s} = 1 -  \frac{6\sum{d_i^{2}}}{n(n^{2}-1)} 
%\end{equation*}
%where $d_{i}$ is the difference between the rank for system$_{i}$ and n is the number of systems. The possible values of range between 1 (where all systems are ranked in the same order) and -1 (where the systems are ranked in the reverse order). 
%
%\begin{equation*}
%r_{s} = r_{p}(r(H_{i}),r(M_{i}))
%\end{equation*}


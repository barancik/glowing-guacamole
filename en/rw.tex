\chapter{Related Work}


\section{Machine Translation Evaluation Metrics}
\subsection{Traditional}



\subsection{With paraphrase support}




\section{Generating additional references}
\subsection{Paraphrasing}
Targeted paraphrasing for MT evaluation is introduced in~\citet{kauchak}. They 
focus on lexical substitution in Chinese-to-English translations. 
They select all pairs of words for which one word appears in a reference sentence,
second word in a hypothesis (the MT output), but none of them in both. They keep 
only pairs of synonymous words, i.e. words appearing in the same WordNet 
\cite{wordnet} synset. Each such a pair of words was further contextually evaluated. 
For every confirmed synonym, a new reference sentence is created by placing it to the
reference sentence on the position of its synonym.


\subsection{Machine Translation}
\cite{yoshimura_2019a} shows that metric correlation can be improved by using
off-the-shelf MT systems (here Google Translate, Bing Translate). The 
source sentences are translated and the translation filtered using BERT-based classifier
trained on Microsoft Research Paraphrase Corpus (MRPC) \citep{dolan_2005}.  SentBLEU calculated using the original reference sentence + unfiltered 
pseudo-references outperforms baseline (using  only the original reference sentence) 
for majority of language pairs (XX->EN).

\cite{albrecht_2008} use translations from off-the-shelf MT systems (Systran, 
GoogleMT, Moses) as pseudo-references. MT systems are then evaluated directly
(by adding the pseudo-references and computing BLEU and METEOR) and using
newly leared metric (extracting features based on similarity with reference translation
 and monolingual fluency and training SVM to predict sentence score). This learned 
 metrics performs better than BLEU/METEOR with single references for all languages 
but German and adding new references generally improves correlation of the standard
 evaluation metrics for most of language pairs.